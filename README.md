# unbenchmarkable-benchmarks
The idea is this: code benchmarks force any deep learning model to exercise lazy hacks and fallbacks on production code. 
Methods like:
- curated high-quality datasets,
- RLHF for principled reasoning, 
- synthetic code generation vetted by experts

inevitably will converge towards lazily hacking the benchmarks since they are using the benchmarks for advertizing of "hey our new model BEAT other models by 3%!".

(per Goodhart's Law, (e.g., arXiv:2412.15004 on LLM code security risks; ICLR 2025 on benchmark cheating))

Lazily hacking is good for academic shallow tasks they spam the students with at non-frontier universities, but horrible for production code.

Hence the need for the benchmarks.

Particularly, since Elon Musk had the courage to not be another "Hey we beat others by 3%" guy with https://x.com/elonmusk/status/2023308259491549191?s=20

# How to quantify what is unbenchmarkable as a benchmark?
I've got some ideas but I currently work on 7 other papers so I will postpone that. If you want to collab and have some time to spare, create an issue and I will share the ideas and even give away the first author if you can get this done.
